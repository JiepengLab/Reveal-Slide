<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

  <title>Neural Network</title>
  <link rel="shortcut icon" href="./favicon.ico" />
  <link rel="stylesheet" href="./dist/reset.css" />
  <link rel="stylesheet" href="./dist/reveal.css" />
  <link rel="stylesheet" href="./dist/theme/simple.css" id="theme" />
  <link rel="stylesheet" href="./css/highlight/github.css" />
  <link rel="stylesheet" href="./assets/custom.css" />


</head>

<body>
  <div class="reveal">
    <div class="slides">
      <section data-markdown>
        <script type="text/template">

## Chapter 8 神经元的数学模型

<div class="fragment">
<div class="three-line">
<center>目录</center>

|章节 ||标题 |
|:--:|:--:|:--:|
|8.1|...|神经网络与数学模型|
|8.2.1|...|逻辑回归（Logistic Regression）|
|8.2.2|...|感知机（Perceptron）|
|8.3|...|程序演示|

</div>
</div>

网站地址：[slide.jiepeng.tech/NN](https://slide.jiepeng.tech/NN)

小组成员：金杰鹏、田宇灼、朱宸、杨秉润、方博真、刘彦翔

</script>
      </section>
      </section>
      <section>
        <section data-markdown>

          <script type="text/template">
  ## 8.1 神经网络与数学模型
  <div class="fragment">

  在线性回归问题中，我们假设输出 $y$ 是输入 $\vec x$ 的线性函数，即 $y = \vec x^T \cdot \vec w\quad\quad$，其中 $\vec w$ 为权值向量
  </div>
  <div class="fragment">


  我们的目标是根据给定的 $\vec x_i$ 和 $y_i$ ，找到一个**最合适**的权值向量 $\vec w$ ，使得函数 $\vec x^T \cdot \vec w$ 和 $y_i$ 拟合的最好

  </div>

  <div class="fragment">
  
注意在上面的拟合中，我们得到的结果 $\vec x^T \cdot \vec w$ 是连续分布的，但如果我们的 $y_i$ 并不是连续分布，而是只取 $0$ 和 $1$ 呢？
          
  </div>
                  </script>
        </section>
        <section data-markdown>

          <script type="text/template">
              ## 8.1 神经网络与数学模型
              <div class="fragment">
  
              我们将 $y_i$ 取值为 $0$ 和 $1$ 的这样一类问题叫做**二分类问题**，此时 $y_i$ 也叫分类标签(lable)
              </div>
              <div class="fragment">
  
              此时，我们通过引入**激活函数**(activation function)的方式，在线性回归的基础上得到二分类结果
              </div>
              <div class="fragment">
  
              激活函数的实质就是一个 if-else 分类，我们设定阈值 $T$ ，如果线性回归的值 $\vec x^T \cdot w > T$ ，则我们认为结果 $y_i = 1$ ，否则为 $0$
              </div>
              <div class="fragment">
  
              上面的过程和生物中神经元激活的过程十分相似，故我们从神经元入手详细的介绍这一过程
              </div>
            </script>

        </section>

      </section>
      <section>
        <section data-markdown>
          <script type="text/template">

## 8.1.1 神经元与其数学描述
<div class="fragment">

神经元的本质是一个函数，接受输入，得到输出
</div>
<div class="fragment">

多层神经网络就是多层复合函数，下面我们介绍最简单的线性层和激活层
</div>
<div class="fragment">

  ![image-20231202164825786](markdown-img/note.assets/image-1.png)
</div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">


## 8.1.1 神经元与其数学描述

<div class="fragment">

- 输入向量 (inputs vector)：$x = (1,x_1,x_2,...,x_{m-1})^T$
- 权重向量 (weights vector)：$w = (w_0,w_1,w_2,...,w_{m-1})^T$
- 激活函数 (activation function)：$f$
</div>
<div class="fragment">

<p>
  <font size="5">
我们用上面三个量的组合来描述一个神经元的激活过程
$$
y = f(x^T\cdot w) =w_0+\sum_{i=1}^{m-1}w_ix_i 
$$
这里 $w_0$ 也被称作偏移项(bias)，上式中 $x^T\cdot w$ 的实质就是对这两个向量做点乘

</font></p> 
</div>
<div class="fragment">
  
<p>
  <font size="5">

$$
x \xrightarrow{w} x^T\cdot w \xrightarrow{f}y
$$

这里我们把第一层叫做`线性层`，第二层叫做`激活层`
</font></p> 

</div>
</script>
        </section>

      </section>
      <section>
        <section data-markdown>
          <script type="text/template">


## 8.1.2 功能例子：二分类

<div class="fragment">

现在，我们希望我们的神经元能够对输入进行二分类。
</div>

<div class="fragment">

  也就是说
  </div>
  <div class="fragment">

  <div class="three-line">
    
    |参数 |形式 |
    |:--:|:--:|
    |Input|$x = (1,x_1,x_2,...,x_{m-1})^T$|
    |Output|$$\text{if}\quad i \in C_{yes}, \quad y_i = 1$$ $$\text{if}\quad i \in C_{no}, \quad y_i = 0$$|
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    | | |
    || ________________________________________|

    </div>
  </div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.1.2 功能例子：二分类

<div class="fragment">

为了实现分类的效果，我们需要基于我们的数据集 

$$(x_i,y_i)\in \mathbb{R}^m\times \text{{0,1\}}$$ 

来训练出我们需要的权重向量 $w$，使得对于每一个 $x_i\quad$，我们都能够得到正确的输出 $y_i\quad$。

</div>

<div class="fragment">

显然，不同的激活函数训练出来的权重向量 $w$ 也是不同的。

</div>

<div class="fragment">

  我们来考察一些激活函数
  
  </div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## 8.1.3 阀值激活函数

<div class="fragment">

激活函数其实就是将 $x^T\cdot w\quad$（接下来记作 $z\quad$） 映射到一个新的值。
  
</div>

<div class="fragment">

最简单的一种就是阀值激活函数（threshold activation function），即
  
</div>

<div class="fragment">
$$
\begin{equation}
f_T(z)=\left\{
\begin{aligned}
1 \quad z\geq 0\\
0 \quad z<0\\
\end{aligned}
\right
.
\end{equation}$$
</div>

<div class="fragment">

由于其不可导，我们可以引入其他函数函数来近似阀值激活函数
</div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.1.4 Sigmoid 函数

<div class="fragment">

  Sigmoid 函数是一种常用的激活函数，其定义如下
</div>

<div class="fragment">

$$
f_S(z)=\frac{1}{1+e^{-z}}
$$
</div>
<div class="fragment">

该如何实现二分类呢？
</div>

<div class="fragment">

  通过简单的推导，我们可以知道
</div>
<div class="fragment">

- Sigmoid 函数的值域为 $(0,1)$

</div>
<div class="fragment">

- Sigmoid 函数关于点 $(0,\frac{1}{2})$ 对称

</div>
<div class="fragment">

$$
y=
\begin{cases}
1 \quad f_S(x^T\cdot w)\geq \frac{1}{2}\\\\
0 \quad \text{else}
\end{cases}$$
</div>

</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## 8.1.5 两种不同激活函数

![Alt text](markdown-img/note.assets/image-2.png)

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
              ## 8.1.5 两种不同激活函数

<div class="fragment">

不同的激活函数对应着不同的神经模型
</div>
<div class="fragment">

<div class="three-line">
<center></center>

|激活函数 ||神经模型 |
|:--:|:--:|:--:|
| $f_S(x^T\cdot w)$ |...|逻辑回归（LR-Logistic Regression）|
| $f_T(x^T\cdot w)$ |...|感知机（Perceptron）|
</div>
<div class="fragment">

接下来，我们将逐一介绍这两种激活函数对应的回归类型，及它们是如何计算确定权重向量 $w$ 的。

</div>
</script>
        </section>
      </section>

      <section>

        <section data-markdown>
          <script type="text/template">
## 8.2.1.1 Sigmoid函数的性质

<div class="fragment">

`Sigmoid` 函数 $\sigma(x)$ 定义如下
</div>
<div class="fragment">

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

</div>
<div class="fragment">

其图像如下
</div>
<div class="fragment">

![image-20231202170845027](markdown-img/note.assets/image-20231202170845027.png)
<center>
    sigmoid 函数图像
</center>
</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.1 Sigmoid函数的性质

<div class="fragment">

我们可以进一步的计算其反函数来考察这个激活函数的性质

</div>
<div class="fragment">

$$
p =\sigma(z)= \frac{1}{1+e^{-z}} \Rightarrow z = \ln \frac{p}{1-p}
$$

</div>
<div class="fragment">

上式右边的等式可以这样解释：若为 $p$ 为某事件发生的概率，$1-p$ 为其不发生的概率，那么我们用 $z$ 来衡量二者比值的大小，从而相对的衡量这个事件发生概率的大小

</div>
<div class="fragment">

该反函数通常也叫做 logit 函数，即 $z = \ln {\frac{p}{1-p}} = \text{logit(p)}$

</div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.1 Sigmoid函数的性质

<div class="fragment">

若将 `Sigmoid` 函数作为激活函数代入我们的神经网络中，即得到

</div>
<div class="fragment">

$$
y = \sigma(x^T \cdot w) \Rightarrow x^T \cdot w = \text{logit}(y)
$$

</div>
<div class="fragment">

即可以看作我们通过权值 $w$ 将 $x$ 线性的变换成激活的强度 $\text{logit}(y)$
</div>

<div class="fragment">

实际上，也就是说 $p(y=1|x)\quad$ 的对数几率是关于 $x$ 的线性函数，这其实就是 Logistic 回归模型的实质。
</div>


<div class="fragment">

我们其实就是使用线性回归模型去逼近二分类任务标签值为 $1$ 的对数几率。
</div>

</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.2 极大似然估计

<div class="fragment">

现在我们知道了二分类问题可以用 Logistic 模型求解，那么现在的问题就是已知数据集 $D\quad$，我们要如何训练出合适的 $w\quad$。
</div>


<div class="fragment">

这就需要引入极大似然估计。
</div>


<div class="fragment">

本质上，我们通过引入似然函数将原先的求 $w$ 的问题转化为以对数似然函数为目标函数的最优化问题。

</div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">

<div class="fragment">

首先我们可以假设标签是符合伯努利分布 $\mathcal{B}(p)\quad$，其中 $p$ 就是我们激活函数所给出的值。
</div>


<div class="fragment">

那么对于某一个标签在权重取成 $w$ 的概率分布函数就为

</div>


<div class="fragment">

$$
\begin{aligned}
p(y_i| w) &= p ^ {y_i} (1 - p) ^ {1 - y_i} \\\\
&= (f_S(x_i^T \cdot w))^{y_i} \cdot (1 - f_S(x_i ^ T \cdot w))^{1 - y_i}
\end{aligned}
$$
</div>


<div class="fragment">
  
我们在假设数据集之间的数据是独立的，即每组 $\{x_i, y_i\}$ 相互独立，那么
</div>


<div class="fragment">

$$
p(\boldsymbol{y} | w) = \prod_{i = 1} ^ n p(y_i | w)
$$

</div>


<div class="fragment">

我们假设存在这样一个最佳的权重，那么现在就是使这个后验概率 $p(w | \boldsymbol{y})$ 最大
</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">
            <div class="fragment">

利用贝叶斯公式我们可以知道
</div>

<div class="fragment">

$$
p (w | \boldsymbol{y}) = \frac{p(\boldsymbol{y} | w) \cdot p(w)}{p(\boldsymbol{y})}
$$
</div>

</div>

<p class="fragment">
<font size="5">
由于 $w$ 的取值是整个空间，可以假设 $p(w)$ 是均匀分布的，而 $p(\boldsymbol{y})$ 就是 $\boldsymbol{y}$ 的概率分布函数，故想要最大化后验概率就是最大化 $p(\boldsymbol{y} | w)$
</font>

</p>

<div class="fragment">

因此我们就将 $p(\boldsymbol{y} | w)$ 称作似然函数（记作 $L$ ）
</div>

<div class="fragment">

  $$
L(w) = p(\boldsymbol{y} | w)
$$

</div>

<div class="fragment">

  为了便于求解，我们将似然函数两边求解，得到

</div>

<div class="fragment">

  $$
\ln L(w) = \sum_{i = 1} ^ n (y_i \cdot \ln f_S(x_i^T \cdot w) + (1 - y_i) \cdot \ln(1 - f_S(x_i^T \cdot w)))
$$
</div>

<div class="fragment">

我们现在的目标就是最大化这个函数。

</div>


</script>
        </section>
      </section>
      <section>


        <section data-markdown>
          <script type="text/template">
## 8.2.1.3 平均交叉熵 (Average Cross-Entropy) 

</div>

<div class="fragment">

在机器学习中，我们往往用损失函数来描述模型预测错误的程度。在 Logistic 回归中，我们也是类似的，将似然函数和一个损失函数相对应。如此，将最大化似然函数转化为最小化损失函数，可以利用梯度下降求解。
</div>

<div class="fragment">

  非常简单，我们只需要将函数取反，同时，为了消除数据数量对损失的印象，我们对所有的和取平均数，即

</div>
<div class="fragment">

$$
\min_w H(w) = \frac{1}{n} \cdot \sum_{i = 1} ^ n H_i(w)
$$
</div>

<div class="fragment">

其中
$
H_i(w) = - y_i \cdot \ln f_S(x_i^T \cdot w) -  (1 - y_i) \cdot \ln(1 - f_S(x_i^T \cdot w))
$
</div>

<div class="fragment">

这其实就是信息论中的交叉熵函数。

</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.3 平均交叉熵 (Average Cross-Entropy) 


<div class="fragment">

我们求出交叉熵函数的梯度
</div>

<div class="fragment">

$$\begin{aligned}\nabla H_i(w) &= (f_S(x_i \cdot w) - y_i) \cdot x_i \\\\
\nabla ^ 2 H_i(w) &= f_S(x_i \cdot w) \cdot (1 - f_S(x_i)) \cdot x_i \cdot x_i^T\end{aligned}$$
</div>

<div class="fragment">

由于 $x_i \cdot x_i^T$ 是半正定的，且对于每一个 $\nabla^2H_i$ 都成立，因此 $H$ 是凸函数，我们可以采用梯度下降求解。
</div>

</script>
        </section>
      </section>
      <section>

        <section data-markdown>
          <script type="text/template">
## 8.2.1.4(1) 梯度下降法

<div class="fragment">

我们采用梯度下降法来寻找使 $H(w)$ 最小的 $w$.
</div>
<div class="fragment">

其基本思想是对某一个更新步 $t$，其对应的 $w = w(t)\quad$.
</div>

<div class="fragment">

为了计算目标函数 $H(w)$ 的极小值，我们需要得到其在这一时刻的梯度 $\nabla H(w(t))\quad$，随后用下面的方程来更新我们的 $w(t)$
</div>

<div class="fragment">

$$
w(t+1) = w(t) - \eta \nabla H(w(t))
$$
</div>

<div class="fragment">

其中 $\eta$ 是一个待定的参数，称为 **步长或学习率**
</div>

<div class="fragment">

因为 $H(w)$ 是各个交叉熵函数的平均值，故由梯度的线性性，我们知道
$
\nabla H(w) = \frac 1n \sum_{i=1}^n \nabla H_i(w)
$
</div>

<div class="fragment">

将其代入更新方程迭代计算即可
</div>

</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.4(2)随机梯度下降法
<div class="fragment">

在计算上面的梯度下降法时，我们需要计算出所有交叉熵函数的梯度再取平均，在计算上非常麻烦，故我们引入了 **随机梯度下降法 (SGD)**
</div>

<div class="fragment">

随机梯度下降法的要点是每次更新时 **随机地** 从 $i=1,2,...,n\quad$ 中选择 **一个** 交叉熵函数的梯度 $\nabla H_i(w)\quad$，用它来替代整个平均交叉熵函数，即将更新方程改为
</div>

<div class="fragment">
  
  $$
w(t+1) = w(t) - \eta \nabla H_i(w(t))
$$
</div>

<div class="fragment">

代入上面我们求得的梯度公式即为
</div>

<div class="fragment">


$$
w(t+1) = w(t)-\eta [\sigma(x_i^T\cdot w) - y_i] \cdot x_i
$$
</div>

<div class="fragment">


下面我们来证明这一方法的收敛性（可行性）
</div>


</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.5 收敛性证明
</div>

<div class="fragment">

假设最终的解为 $w$，即满足
</div>

<div class="fragment">

  $$
w = \text{argmin} \ H(w)
$$
</div>

<div class="fragment">

我们考察 $t+1$ 轮更新得到的 $w(t+1)$ 与 $w$ 之间的二维欧式距离 $\|\cdot\|_2\quad$，下面简写为 $|\cdot |$
</div>

<div class="fragment">

代入上面的更新方程得到
</div>

<div class="fragment">
<font size="5">

$$
\begin{aligned}
|w(t+1)-w|^2 &= |w(t)- \eta \nabla H_i(w(t)) -w| ^2\\\\
&=(w^T(t)- \eta \nabla^T H_i(w(t)) -w^T)\cdot(w(t)- \eta \nabla H_i(w(t)) -w)\\\\
&=|w(t)-w|^2 - 2\eta \nabla^TH_i(w(t))(w(t)-w)+ \eta^2 |\nabla^TH_i(w(t))|^2 \quad (3)\qquad\quad
\end{aligned}
$$
</font>

</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明

<div class="fragment">


对最后一项我们有

</div>

<div class="fragment">

$$
\begin{aligned}
|\nabla^TH_i(w(t))| &= |[\sigma(x_i^T\cdot w(t)) - y_i] \cdot x_i|\\\\
&=|\sigma(x_i^T\cdot w(t)) - y_i| \cdot |x_i|
\end{aligned}
$$

</div>

<div class="fragment">

因为$0\leq \sigma(x_i^T\cdot w),y_i\leq 1\qquad\quad$，故$|\sigma(x_i^T\cdot w) - y_i| \leq 1\quad\quad$，即

</div>

<div class="fragment">

$$
|\nabla^TH_i(w(t))|=|\sigma(x_i^T\cdot w(t)) - y_i| \cdot |x_i| \leq |x_i|
$$

</div>

<div class="fragment">

令 $G$ 为所有 $|x_i|$ 中的最大值，则有
</div>

<div class="fragment">

  $$
G = \max_{i=1,2,...,n}|x_i|\\
|\nabla^TH_i(w(t))| \leq G
$$

</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明


<div class="fragment">



故我们找到了 $(3)$ 式最后一项的一个上界，即化为
</div>

<div class="fragment">

  $$
|w(t+1)-w|^2 \leq|w(t)-w|^2 - 2\eta \nabla^TH_i(w(t))(w(t)-w)+ \eta^2 G^2 \quad (4)\quad\quad\quad\quad\quad
$$

</div>

<div class="fragment">

  由于 $(4)$ 式中的 $\nabla^TH_i(w(t))\quad$ 是每次在 $i = 1,2,...,n\quad$ 中随机选取的，我们不妨考虑 $(4)$ 式的期望
</div>

<div class="fragment">
<font size="5">

$$
  \begin{aligned}
E(|w(t+1)-w|^2)
&\leq E(|w(t)-w|^2) - 2\eta E(\nabla^TH_i(w(t))) \cdot (w(t)-w) + \eta^2G^2\\\\
&=E(|w(t)-w|^2) + 2\eta E(\nabla^TH_i(w(t))) \cdot (w-w(t)) + \eta^2G^2 \quad(5)\quad\quad\qquad
\end{aligned}
$$
</font>

</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明

<div class="fragment">

我们先来考虑中间项 $E(\nabla^TH_i(w(t)))\quad\quad$，如果我们在 $\{1,2,...,n\}\quad$ 中均匀的选取 $i$，即每个数被选到的概率都是 $\frac 1n$，则每个梯度 $\nabla ^TH_i(w)$ 被选到的概率也是 $\frac1n$，那么此时的期望即为
</div>

<div class="fragment">

$$
\begin{aligned}
E(\nabla^TH_i(w(t))) &= \frac 1n \sum_{i=1}^n \nabla^TH_i(w(t)) \\\\&= \nabla^T(\frac 1n\sum_{i=1}^nH_i(w(t)) ) \\\\&= \nabla^T H(w(t))
\end{aligned}
$$
</div>

<div class="fragment">

故 **随机选取梯度的期望正好就是平均交叉熵函数的梯度**
</div>



</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## 8.2.1.5 收敛性证明

<div class="fragment">

上面我们证明过交叉熵函数是凸函数，故由凸函数的性质我们有
</div>

<div class="fragment">

$$
H(w) \geq H(w(t))  + \nabla^TH(w(t)) \cdot (w-w(t))
$$
</div>

<div class="fragment">
  
  *这里可以理解为 $f(a) \geq f(b) + f'(b)(b-a)$*
</div>

<div class="fragment">
  
那么整个中间项 $E(\nabla^TH_i(w)) \cdot (w-w(t))\quad\quad$ 即满足
</div>

<div class="fragment">
  
$$
\begin{aligned}
E(\nabla^TH_i(w)) \cdot (w-w(t)) & \leq \nabla^TH(w)\cdot (w-w(t))\\\\ &\leq H(w) - H(w(t))
\end{aligned}
$$

</div>

<div class="fragment">
  
综上我们有：

</div>
<div class="fragment">
  <font size="5">
  
  $$
  E(|w(t+1)-w|^2)\leq E(|w(t)-w|^2) + 2\eta (H(w) - H(w(t))) + \eta^2G^2
  $$
  </font>

  </div>
  

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明


<div class="fragment">

移项整理得到
</div>

<div class="fragment">

$$
H(w(t)) - H(w) \leq  \frac 1\eta \cdot \frac {E(|w(t)-w|^2) - E(|w(t+1)-w|^2)}2 + \eta \cdot \frac {G^2}2\quad \quad \quad \quad 
$$  
</div>

<div class="fragment">

不难发现上面这个不等式的右边是一个递推差分的形式，故我们将上式对 $t=1,2,...,T\quad $ 求和得到
</div>

<div class="fragment">
<font size="5">
$$
\begin{aligned}
\sum\limits_{i=1}^T[H(w(t)) - H(w)] &\leq \frac 1\eta \cdot \frac {E(|w(1)-w|^2) - E(|w(T+1)-w|^2)}2 + \eta \cdot \frac {TG^2}2\quad\quad\quad\\\\
&\leq \frac 1\eta \cdot \frac {E(|w(1)-w|^2)}2 + \eta \cdot \frac {TG^2}2
\end{aligned}
$$

</font>

</div>

<div class="fragment">

因为我们的迭代起点 $w(1)$ 是一个确定值（相对于迭代过程而言），故 $E(|w(1)-w|^2) = |w(1)-w|^2$
</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明

<div class="fragment">

则上式化为
</div>

<div class="fragment">


$$
\sum_{i=1}^TH(w(t)) - T\cdot H(w) \leq \frac 1\eta \cdot \frac {|w(1)-w|^2}2 + \eta \cdot \frac {TG^2}2$$$$
\frac 1T\sum_{i=1}^TH(w(t)) - H(w) \leq \frac 1\eta \cdot \frac {|w(1)-w|^2}{2T} + \eta \cdot \frac {G^2}2
$$

</div>

<div class="fragment">

因为 $H(w)$ 是一个凸函数，故我们有

</div>

<div class="fragment">

$$
\frac 1T\sum_{i=1}^TH(w(t)) \geq H(\frac 1T\sum_{i=1}^Tw(t))
$$

</div>

<div class="fragment">

*这里可以理解为* $\frac{f(a)+f(b)}2 \geq f(\frac{a+b}2)$

</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明

<div class="fragment">


取 $\bar w(T) = \frac 1T\sum_{i=1}^Tw(t)\quad\quad\quad$，则有

</div>

<div class="fragment">


$
H(\bar w(T)) - H(w) \leq \frac 1T\sum\limits_{i=1}^TH(w(t)) - H(w) \leq \frac 1\eta \cdot \frac {|w(1)-w|^2}{2T} + \eta \cdot \frac {G^2}2
$

</div>

<div class="fragment">

由于步长 $\eta$ 可以自由选择，这里我们选择使不等式右端最小的$\eta\quad$，即根据均值不等式，

</div>

<div class="fragment">

$$
\eta = \text{argmin} \frac 1\eta \cdot \frac {|w(1)-w|^2}{2T} + \eta \cdot \frac {G^2}2 =  \frac{|w(1)-w|}G \cdot \sqrt{\frac 1T}
$$

</div>

<div class="fragment">

此时有不等式

</div>

<div class="fragment">

$$
H(\bar w(T)) - H(w) \leq |w(1)-w| \cdot G \cdot \sqrt{\frac 1T}
$$

</div>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.1.5 收敛性证明

<div class="fragment">

即我们通过随机梯度下降法得到的 $H(\bar w(T))\quad$ 和极小值 $H(w)$ 的差存在上界，其中 $|w(1)-w|\quad$ 和 $G$ 都不依赖于迭代过程，故有

</div>

<div class="fragment">

$$
\lim _{T\to \infty} H(\bar w(T)) = H(w)\\
且H(\bar w (T)) - H(w) = O(\sqrt{\frac 1T})
$$
</div>

<div class="fragment">

由上面的结论我们知道，随机梯度下降法可以用来求平均交叉熵函数的极小值，并求出我们的参数 $w$
</div>

</script>
        </section>
      </section>


      <section data-markdown>
        <script type="text/template">
## 8.2.2 感知机 | Perceptron



<div class="fragment">


现在让我们考虑一下<font color="DarkRed">阈值激活函数(threshold activation function)</font>的情况。

</div>

<div class="fragment">


相应的神经模型被称为<font color="DarkBlue">感知机(perceptron)</font>。

</div>

<div class="fragment">


通过模拟<font color="DarkRed">随机梯度下降(SGD)</font>，我们推导出了<font color="DarkBlue">Rosenblatt 学习(Rosenblatt learning)</font>，它在有限的时间内识别出一个线性分类器。

</div>

<div class="fragment">


值得注意的是，如果数据样本不是线性可分的，Rosenblatt学习就会失败。这种困难可以通过具有<font color="DarkBlue">隐藏层(hidden layers)</font>的感知机来克服。

</div>

</script>
      </section>
      <section>


        <section data-markdown>
          <script type="text/template">

## 8.2.2.1 Rosenblatt 学习



<div class="fragment">

- 我们假设数据样本 $(x_i, y_i)\in \mathbb{R}^m \times \{0, 1\}, i=1, \ldots, n\quad\quad\quad$ 是严格线性可分的。这意味着存在一个权值向量 $w\in \mathbb{R}^m$，使得它对所有的 $i=1, \ldots, n$ 都满足下面的条件：
</div>
<div class="fragment">

$$
w^T\cdot x_i > 0, \text{ if and only if } y_i = 1, w^T\cdot x_i < 0, \text{ if and only if } y_i = 0.\quad\quad\quad
$$
</div>

<p class="fragment">
也就是说，我们有一个<font color="DarkRed">超平面</font></p>
<p class="fragment">
$$
H=\{x\in \mathbb{R}^m: w^T\cdot x = 0\}
$$
</p>
<p class="fragment">
区分了 $C_{yes}$ 和 $C_{no}$
</p>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

![Alt text](./markdown-img/note.assets/image.png)

<p class="fragment">
这里的 $w$ 就是一个<font color="DarkRed">线性分类器（linear classifier）</font>。
</p>

<p class="fragment">
我们该如何合理地找到这样的 $w$ 呢？
</p>

</script>
        </section>


        <section data-markdown>
          <script type="text/template">

## SGD Update


- 我们可以使用<font color="DarkRed">随机梯度下降 (SGD)</font>来找到这样的 $w\quad$。

$$w(t+1)=w(t)-\eta\cdot (f_S(x_i^T\cdot w(t))-y_i)\cdot x_i$$

<p class="fragment">
· 将 SGD 中的 <font color="DarkRed">Sigmoid 激活函数</font> $f_S$ 替换为<font color="DarkRed">阈值激活函数</font> $f_T$
</p>
<p class="fragment">
· 将步长 $\eta$ 设为 1。
</p>
<p class="fragment">
· 便得到了<font color="DarkBlue">Rosenblatt 学习</font>，也被称为<font color="DarkBlue">感知器算法(perceptron algorithm)</font>。
</p>

<p class="fragment">
$$w(t+1)=w(t)-(f_T(x_i^T\cdot w(t))-y_i)\cdot x_i$$
</p>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 感知器算法



<p class="fragment">
① 感知器算法是由 Rosenblatt 于1957年在康奈尔航空实验室发明的。
</p>

<p class="fragment">
② 感知器的初衷是作为一种<font color="DarkBlue">机器</font>而非<font color="DarkRed">程序</font>。虽然它的首次实现是在软件中，但后来又被实现为硬件——“Mark 1 perception”。
</p>

<p class="fragment">
③ 1958年，纽约时报报道感知器被预期为 “<font color="DarkRed">一台电子计算机的胚胎</font>，[The Navy]预期它将能够<font color="DarkBlue">行走、讲话、看见、写作、复制自身</font>并且<font color="DarkBlue">意识到自己的存在</font>”。
</p>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

我们再请回之前的 Rosenblatt 学习的更新规则：

<p class="fragment">
$$w(t+1)=w(t)-(f_T(x_i^T\cdot w(t))-y_i)\cdot x_i$$
</p>

<p class="fragment">
为了理解感知器的重要性，让我们重写一下上式：
</p>
<p class="fragment">
对于分类<font color="DarkBlue">正确</font>的样本，有 $f_T(x_i^T\cdot w(t))=y_i\quad\quad$，权值向量 $w$ 不会改变：
</p>
<p class="fragment">
$$w(t+1)=w(t)$$
</p>
<p class="fragment">
对于分类<font color="DarkRed">不正确</font>的样本，有 $f_T(x_i^T\cdot w(t))\neq y_i\quad\quad$，权值向量 $w$ 会改变：
</p>
<div class="fragment">

$$w(t+1)=\begin{cases}w(t)+x_i, & \text{if } f_T(x_i^T\cdot w(t))=0 \text{ and } y_i=1\\\\w(t)-x_i, & \text{if } f_T(x_i^T\cdot w(t))=1 \text{ and } y_i=0\end{cases}$$
</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

<div class="fragment">

第二种情况可以轻易地用神经学习的术语进行解读。
</div>
<div class="fragment">

  - 一次<font color="DarkRed">错误分类</font>会引起<font color="DarkBlue">权重变化</font> $w(t+1)−w(t)\quad\quad$，等于<font color="DarkBlue">带符号输入</font> $±x_i\quad$
</div>

<div class="fragment">

  - 这里的符号取决于我们是错误地将输入分配给了类别 $C_{yes}$ 还是 $C_{no}$
</div>

<div class="fragment">

或者，我们从向量的视角考虑
</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<div class="fragment">

![Alt text](./markdown-img/note.assets/image-3.png)
</div>

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

![Alt text](./markdown-img/note.assets/image-4.png)

</script>
        </section>
        <section data-markdown>
          <script type="text/template">

![Alt text](./markdown-img/note.assets/image-5.png)

</script>
        </section>
      </section>
      <section>


        <section data-markdown>
          <script type="text/template">
<font align="left">

## 8.2.2.2 Rosenblatt 学习的收敛性

接下来我们证明Rosenblatt 学习会在有限多步后停止，并为我们提供一个线性分类器w。 

为此，我们假设在Rosenblatt 学习中指标 $i = 1，…，n\quad$ ，按循环顺序变化。考虑一般性时，我们可以在分析中假设所有步骤 $i = 1，…，n\quad$ 的类型为错误分类（而如果存在正确分类的中间步骤，我们只考虑分类错误的更新并重标序号）  

我们以 $w(1) = 0\quad$ 开始迭代过程。 
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<font size="6">
<p class="fragment">
首先可以写出:</p>  
<p class="fragment">
$$\|w(t+1)\|_2^2=\|w(t)\pm x_i\|_2^2=\|w(t)\|_2^2\pm2x_i^T\cdot w^T(t)+\|x_i\|_2^2.$$
</p>  
<p class="fragment">
对于最后一项可以一致有界进行放缩：    
</p>
<p class="fragment">
$$\|x_i\|_2\leq\max_{i=1,...,n}\|x_i\|_2=G.$$
</p>
<p class="fragment">
基于我们对错误分类的处理方法，中间项是始终非正的：  
</p>
<p class="fragment">
$$\begin{aligned}+x_i^T\cdot w(t)&<0\text{ iff }f_T\left(x_i^T\cdot w(t)\right)=0,\\\\-x_i^T\cdot w(t)&\le0\text{ iff }f_T\left(x_i^T\cdot w(t)\right)=1.\end{aligned}$$ 
</p>

<p class="fragment">
综上，我们可以得到：</p>
<p class="fragment">
$\|w(t+1)\|_2^2\leq\|w(t)\|_2^2+G^2\leq\ldots\leq\underbrace{\|w(1)\|_2^2}_{=0}+t\cdot G^2=t\cdot G^2.$
</p>  
</font>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<p class="fragment">
进一步地，对于线性分类器 $w$ 我们有：</p><p class="fragment">$$w^T\cdot w(t+1)=w^T\cdot(w(t)\pm x_i)=w^T\cdot w(t)\pm x_i^T\cdot w.$$</p>
<p class="fragment">
基于我们对错误分类的处理方法，可知最后一项是非负的：</p>    
<p class="fragment">
$$\begin{aligned}+x_i^T\cdot w&>0\text{ iff }y_i=1,\\\\-x_i^T\cdot w&>0\text{ iff } y_i=0.\end{aligned}$$    
</p>
<p class="fragment">
那么它就可以被一个正常数限定下界：</p>
<p class="fragment">
$$\pm x_i^T\cdot w\geq\min_{i=1,\dots,n}\pm x_i^T\cdot w=g>0.$$
</p> 
<p class="fragment">
综上，我们可以得到：  </p>
<p class="fragment">
$$w^T\cdot w(t+1)\geq w^T\cdot w(t)+g\geq\ldots\geq w^T\cdot\underbrace{w(1)}_{=0}+t\cdot g=t\cdot g.$$
</p>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
由 Cauchy-Schwarz 不等式：  
$$t\cdot g\leq w^T\cdot w(t+1)\leq\|w\|_2\cdot\|w(t+1)\|_2\leq\|w\|_2\cdot\sqrt{t\cdot G^2}.$$  
于是我们得知更新次数 t 是存在上界的：  
$$t\leq\|w\|_2^2\cdot\frac{G}{g}.$$
到此我们便证明了Rosenblatt 学习是会在有限多步后停止的，具有收敛性。  
</script>
        </section>
      </section>
      <section>

        <section data-markdown>
          <script type="text/template">

## 8.2.2.3 XOR 问题

<p class="fragment">
虽然<font color="DarkBlue">Rosenblatt 学习</font>最初看起来充满希望，但很快就被证明感知器无法对<font color="DarkRed">非线性可分</font>的数据进行分类。
</p>
<p class="fragment">
为了说明这点，我们引用了经典的<font color="DarkRed">XOR 反例</font>。XOR 问题模拟的是一种只有在输入不同时才会输出真值的逻辑运算。让我们通过真值表来描述 XOR，这个表格列出了逻辑表达式在各个函数参数上的函数值：
</p>

<p class="fragment">
$$\begin{array}{c|c|c}\text{input } x &(1,1)&(1,0)&(0,1)&(0,0)\\\hline\text{output } y &0&1&1&0\end{array}$$
</p>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">

## 8.2.2.3 XOR 问题

<div class="fragment">
  <center>
<img src="./markdown-img/note.assets/image-6.png" width="30%"  />
  </center>
</div>

<p class="fragment">
显然，XOR 问题无法用简单的线性感知器进行分类。我们考虑激活函数 $f_T(w_0+w_1x_1+w_2x_2)\quad\quad\quad$，无论 $w_0, \ w_1, \ w_2\quad\quad$ 如何取值，四个点上的分类结果都至少有一个不符合。这说明我们需要一个 <strong>非线性函数</strong> 完成分类。而如果想用神经网络表达一个 <strong>非线性函数</strong>，仅仅两层（输入层和输出层）是远远不够的。
</p>

</script>
        </section>
      </section>
      <section>


        <section data-markdown>
          <script type="text/template">

## 多层感知机

<div class="fragment">

就如同字面意思，多层感知机（Multilayer Perceptron）将神经网络的层数从双层拓展到了三层（甚至可以更多）。中间的既非输入也非输出层的节点属于 **隐藏层**（hidden layer）。
</div>

<div class="fragment">
  <center>
<img src="./markdown-img/note.assets/image-multilayer.png" width="70%"  />
  </center>

</div>
<div class="fragment">

其中 $w_0^{\ell}, w_1^{\ell}, \ldots, w_{m-1}^{\ell} \in \mathbb{R}\quad\quad\quad$ 对应隐藏层的第 $\ell \ (\ell \in [1,k])\quad$ 个节点的 $m$ 个权重。
</div>


</script>
        </section>
        <section data-markdown>
          <script type="text/template">

<div class="fragment">

和之前一样，我们用向量形式
$$x=\left(1, x_1, \ldots, x_{m-1}\right)^T, \quad w^{\ell}=\left(w_0^{\ell}, w_1^{\ell}, \ldots, w_{m-1}^{\ell}\right)^T$$
来表示变量。
<div>
  <div class="fragment">

那么对于隐藏层中的第 $\ell$ 个节点，有 $z_{\ell}=f_T\left(x^T \cdot w^{\ell}\right)$
<div>
  <div class="fragment">

而对于输出层，我们有 $y=v_0+z_1 \cdot v_1+\ldots+z_k \cdot v_k=z^T \cdot v$
<div>
  <div class="fragment">

将隐藏层和输出层合二为一地看，我们得到输出 $y$ 与输入向量 $x$ 的关系 $y=\sum\limits_{\ell=0}^k f_T\left(x^T \cdot w^{\ell}\right) \cdot v_{\ell}$
<div>
  <div class="fragment">

注意这里 $w^1, \ldots, w^k \in \mathbb{R}^m\quad\quad$ 和 $v_1, \ldots, v_k \in \mathbb{R}\quad\quad$ 都是需要进行调整的参量
<div>



</script>
        </section>
        <section data-markdown>
          <script type="text/template">

<p class="fragment">
  现在看来，多了一层之后，我们似乎能更好地拟合 XOR 这个非线性模型了。那对于更为复杂的模型，我们是否需要更多的层数来进行拟合呢？答案是否定的，这个结果令人惊讶。
</p>

<p class="fragment">
根据 <strong>通用近似定理（Universal approximation theorem）</strong> ，人工神经网络具有近似任意连续函数的能力。形式化地讲，考虑模型 
$$y=F(x)$$
其中 $F: K \rightarrow \mathbb{R}\quad$ 是一个连续的映射，定义域 $K \subset \mathbb{R}^m\quad$。对于任意的 $\varepsilon>0\quad$，总存在一组 $w^1, \ldots, w^k \in \mathbb{R}^m, \ v_1, \ldots, v_k \in \mathbb{R}\quad\quad$，使得下式成立：
$$
\left|F(x)-\sum_{\ell=0}^k f_T\left(x^T \cdot w^{\ell}\right) \cdot v_{\ell}\right| \leq \varepsilon \quad \text { for all } x \in K
$$
</p>

<p class="fragment">
不过，实践和理论有所差距。在实际应用时，多层的神经网络依然必不可少。倘若只有三层，我们很难找到合适的 $w$ 与 $v$，并且这二者的规模可能极大。
</p></script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">

## 8.3 程序演示

[二分类问题](https://slide.jiepeng.tech/NN/two_classification.py)

[XOR 问题](https://slide.jiepeng.tech/NN/xor_problem.py)

</script>
      </section>
      </section>
      <section data-markdown>
        <script type="text/template">

<div style="text-align: center;font-size: 30px">
  
  --------
  --------
  --------
  --------
  --------
  --------
  --------
  
  Thanks for your listening!

</div>

<div style="text-align: right">
  
  —— 2023.12.30
</div>


</script>
      </section>
      </section>

    </div>
  </div>



  <script src="./dist/reveal.js"></script>

  <script src="./plugin/markdown/markdown.js"></script>
  <script src="./plugin/highlight/highlight.js"></script>
  <script src="./plugin/zoom/zoom.js"></script>
  <script src="./plugin/notes/notes.js"></script>
  <script src="./plugin/math/math.js"></script>
  <script>
    function extend() {
      var target = {};
      for (var i = 0; i < arguments.length; i++) {
        var source = arguments[i];
        for (var key in source) {
          if (source.hasOwnProperty(key)) {
            target[key] = source[key];
          }
        }
      }
      return target;
    }

    // default options to init reveal.js
    var defaultOptions = {
      controls: true,
      progress: true,
      history: true,
      center: true,
      transition: 'default', // none/fade/slide/convex/concave/zoom
      slideNumber: true,
      plugins: [
        RevealMarkdown,
        RevealHighlight,
        RevealZoom,
        RevealNotes,
        RevealMath
      ]
    };

    // options from URL query string
    var queryOptions = Reveal().getQueryHash() || {};

    var options = extend(defaultOptions, { "width": 1000, "transition": "fade", "slideNumber": "c/t", "center": false, "transitionSpeed": "fast" }, queryOptions);
  </script>


  <script>
    Reveal.initialize(options);
  </script>
</body>

</html>